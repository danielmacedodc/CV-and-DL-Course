{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea7f2c3e-72ec-4f6f-95fa-7e553e1078fb",
   "metadata": {},
   "source": [
    "# **NLP Project**\n",
    "\n",
    "In this project, it will be used the deep learning model from the paper [Neural Networks for Joint Sentence Classification in Medical Paper Abstracts](https://arxiv.org/pdf/1612.05251.pdf), by replicating the paper [PubMed 200k RCT: a Dataset for Sequential Sentence Classification in Medical Abstracts](https://arxiv.org/abs/1710.06071). */Cheers for the authors. Outstanding job!/*\n",
    "\n",
    "The goal of the dataset was to explore the ability for NLP models to classify sentences which appear in sequential order.\n",
    "\n",
    "In this way, we shall create an NLP model to classify abstract sentences into the role they play (e.g. objective, methods, results, etc) to enable researchers to skim through the literature (hence SkimLit ðŸ¤“ðŸ”¥) and dive deeper when necessary.\n",
    "\n",
    "I'd like to thank **Mr. Daniel Bourke** for helping with this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bcc86c-61fa-46a7-a057-b67aa604978b",
   "metadata": {},
   "source": [
    "## Loading Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "67fa3adb-b919-4721-a6e7-ff6732119000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import help_me\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c363dac-0512-44db-9a7e-5b0cfdcc7d16",
   "metadata": {},
   "source": [
    "## Confirm Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7619695e-3bf4-431f-87cd-828d13a1b4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268e8c63-9ff7-4b6b-a52d-8efc78d3f279",
   "metadata": {},
   "source": [
    "## Get Data\n",
    "\n",
    "Getting the data from the [Github repository](https://github.com/Franck-Dernoncourt/pubmed-rct.git)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b1ad894-a0a6-429e-a487-4a77f1117fd2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5 directories and 1 files in 'pubmed-rct/'.\n",
      "There are 5 directories and 5 files in 'pubmed-rct/.git'.\n",
      "There are 0 directories and 13 files in 'pubmed-rct/.git\\hooks'.\n",
      "There are 0 directories and 1 files in 'pubmed-rct/.git\\info'.\n",
      "There are 1 directories and 1 files in 'pubmed-rct/.git\\logs'.\n",
      "There are 2 directories and 0 files in 'pubmed-rct/.git\\logs\\refs'.\n",
      "There are 0 directories and 1 files in 'pubmed-rct/.git\\logs\\refs\\heads'.\n",
      "There are 1 directories and 0 files in 'pubmed-rct/.git\\logs\\refs\\remotes'.\n",
      "There are 0 directories and 1 files in 'pubmed-rct/.git\\logs\\refs\\remotes\\origin'.\n",
      "There are 2 directories and 0 files in 'pubmed-rct/.git\\objects'.\n",
      "There are 0 directories and 0 files in 'pubmed-rct/.git\\objects\\info'.\n",
      "There are 0 directories and 2 files in 'pubmed-rct/.git\\objects\\pack'.\n",
      "There are 3 directories and 0 files in 'pubmed-rct/.git\\refs'.\n",
      "There are 0 directories and 1 files in 'pubmed-rct/.git\\refs\\heads'.\n",
      "There are 1 directories and 0 files in 'pubmed-rct/.git\\refs\\remotes'.\n",
      "There are 0 directories and 1 files in 'pubmed-rct/.git\\refs\\remotes\\origin'.\n",
      "There are 0 directories and 0 files in 'pubmed-rct/.git\\refs\\tags'.\n",
      "There are 0 directories and 3 files in 'pubmed-rct/PubMed_200k_RCT'.\n",
      "There are 0 directories and 3 files in 'pubmed-rct/PubMed_200k_RCT_numbers_replaced_with_at_sign'.\n",
      "There are 0 directories and 3 files in 'pubmed-rct/PubMed_20k_RCT'.\n",
      "There are 0 directories and 3 files in 'pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign'.\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/Franck-Dernoncourt/pubmed-rct.git\n",
    "for dirpath, dirnames, filenames in os.walk('pubmed-rct/'):\n",
    "    print(f\"There are {len(dirnames)} directories and {len(filenames)} files in '{dirpath}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db78684e-abd3-4cb5-8a9e-72b87e0a1491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files: ['dev.txt', 'test.txt', 'train.txt']\n"
     ]
    }
   ],
   "source": [
    "# Checking the files in the PubMed_20K dataset\n",
    "for _, _, filename in os.walk('pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/'):\n",
    "    print(f\"Files: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da6be3a8-ebd0-4710-be55-7196dbde264f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting the experiments using the 20k dataset with numbers replaced by '@ sign\n",
    "data_dir = 'pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0527277-b84b-4465-a885-ad05bcb5b5ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/dev.txt',\n",
       " 'pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/test.txt',\n",
       " 'pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/train.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check all of the filenames in the target directory\n",
    "filenames = [data_dir + filename for filename in os.listdir(data_dir)]\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da67692-f314-4042-9bc4-7cf05a7fd164",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "Writing a function to read in all of the lines of a target text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19e44972-999c-4c30-bd21-0ade39bf526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a funtion to read the lines of the document\n",
    "def get_lines(filename):\n",
    "    \"\"\"\n",
    "    Reads filename (a text filename) and returns the lines of the text as a list.\n",
    "    \n",
    "    Args:\n",
    "        filename: a string containing the target filepath\n",
    "        \n",
    "    Returns:\n",
    "        A list of strings with one string per line from the target filename.\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        return f.readlines()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5294fd9-03c5-403c-9308-f05e9dcd6c97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['###24293578\\n',\n",
       " 'OBJECTIVE\\tTo investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .\\n',\n",
       " 'METHODS\\tA total of @ patients with primary knee OA were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .\\n',\n",
       " 'METHODS\\tOutcome measures included pain reduction and improvement in function scores and systemic inflammation markers .\\n',\n",
       " 'METHODS\\tPain was assessed using the visual analog pain scale ( @-@ mm ) .\\n',\n",
       " 'METHODS\\tSecondary outcome measures included the Western Ontario and McMaster Universities Osteoarthritis Index scores , patient global assessment ( PGA ) of the severity of knee OA , and @-min walk distance ( @MWD ) .\\n',\n",
       " 'METHODS\\tSerum levels of interleukin @ ( IL-@ ) , IL-@ , tumor necrosis factor ( TNF ) - , and high-sensitivity C-reactive protein ( hsCRP ) were measured .\\n',\n",
       " 'RESULTS\\tThere was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , PGA , and @MWD at @ weeks .\\n',\n",
       " 'RESULTS\\tThe mean difference between treatment arms ( @ % CI ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .\\n',\n",
       " 'RESULTS\\tFurther , there was a clinically relevant reduction in the serum levels of IL-@ , IL-@ , TNF - , and hsCRP at @ weeks in the intervention group when compared to the placebo group .\\n',\n",
       " 'RESULTS\\tThese differences remained significant at @ weeks .\\n',\n",
       " 'RESULTS\\tThe Outcome Measures in Rheumatology Clinical Trials-Osteoarthritis Research Society International responder rate was @ % in the intervention group and @ % in the placebo group ( p < @ ) .\\n',\n",
       " 'CONCLUSIONS\\tLow-dose oral prednisolone had both a short-term and a longer sustained effect resulting in less knee pain , better physical function , and attenuation of systemic inflammation in older patients with knee OA ( ClinicalTrials.gov identifier NCT@ ) .\\n',\n",
       " '\\n',\n",
       " '###24854809\\n',\n",
       " 'BACKGROUND\\tEmotional eating is associated with overeating and the development of obesity .\\n',\n",
       " 'BACKGROUND\\tYet , empirical evidence for individual ( trait ) differences in emotional eating and cognitive mechanisms that contribute to eating during sad mood remain equivocal .\\n',\n",
       " 'OBJECTIVE\\tThe aim of this study was to test if attention bias for food moderates the effect of self-reported emotional eating during sad mood ( vs neutral mood ) on actual food intake .\\n',\n",
       " 'OBJECTIVE\\tIt was expected that emotional eating is predictive of elevated attention for food and higher food intake after an experimentally induced sad mood and that attentional maintenance on food predicts food intake during a sad versus a neutral mood .\\n',\n",
       " 'METHODS\\tParticipants ( N = @ ) were randomly assigned to one of the two experimental mood induction conditions ( sad/neutral ) .\\n']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the training lines\n",
    "train_lines = get_lines(data_dir+'train.txt')\n",
    "train_lines[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93d6c6b-3700-4b2d-a14c-39dd9d514d2a",
   "metadata": {},
   "source": [
    "> Writing a function which turns each of our datasets into a dictionary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46ae16ba-cd00-41c7-a983-a966a0db0293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_with_line_numbers(filename):\n",
    "    \"\"\"\n",
    "    Returns a list of dictionaries of abstract line data.\n",
    "    \n",
    "    Takes in filename, reads its contents and sorts through each line, extracting\n",
    "    things like the target label, the text of the sentence, how many sentences are\n",
    "    in the current abstract and what sentence number the target line is.\n",
    "    \"\"\"\n",
    "    \n",
    "    input_lines = get_lines(filename) # get all lines from filename\n",
    "    abstract_lines = '' # create an empty abstract\n",
    "    abstract_samples = [] # create an empty list of abstracts\n",
    "    \n",
    "    # loop through each line in the target file\n",
    "    for line in input_lines:\n",
    "        if line.startswith('###'): # check to see if the line is and ID line\n",
    "            abstract_id = line\n",
    "            abstract_lines = '' # reset the abstract string if the line is an ID line\n",
    "            \n",
    "        elif line.isspace(): # check to see if line is a new line or empty space\n",
    "            abstract_line_split = abstract_lines.splitlines() # split abstract into separate lines\n",
    "            \n",
    "            # iterate through each line in a single abstract and count them at the same time\n",
    "            for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n",
    "                line_data = {} # create and empty dictionary for each line\n",
    "                target_text_split = abstract_line.split('\\t') #split target label from text\n",
    "                line_data['target'] = target_text_split[0] #get target label\n",
    "                line_data['text'] = target_text_split[1].lower() # get target text and lower it\n",
    "                line_data['line_number'] = abstract_line_number # what number line does the line appear in the abstract\n",
    "                line_data['total_lines'] = len(abstract_line_split) - 1 # how many total lines are there in the target abstract (start from 0)\n",
    "                abstract_samples.append(line_data) # add line data to abstract samples list\n",
    "                \n",
    "        # if the above conditions are not fulfilled, the line contains a labelled sentence\n",
    "        else:\n",
    "            abstract_lines += line\n",
    "    \n",
    "    return abstract_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bf99522-725a-4e50-9109-a47819d69f9d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180040 30212 30135\n",
      "Wall time: 564 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# get data from file and preprocess it\n",
    "\n",
    "train_samples = preprocess_text_with_line_numbers(data_dir+'train.txt')\n",
    "val_samples = preprocess_text_with_line_numbers(data_dir+'dev.txt') # dev is another name for val\n",
    "test_samples = preprocess_text_with_line_numbers(data_dir+'test.txt')\n",
    "print(len(train_samples), len(val_samples), len(test_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "854fd5e2-07dc-41c0-9a81-bf967742e2e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'target': 'OBJECTIVE',\n",
       "  'text': 'to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .',\n",
       "  'line_number': 0,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'a total of @ patients with primary knee oa were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .',\n",
       "  'line_number': 1,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .',\n",
       "  'line_number': 2,\n",
       "  'total_lines': 11}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the first abstract of the training data\n",
    "train_samples[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfcba80-cdd4-4af6-b152-cc559ffcb138",
   "metadata": {},
   "source": [
    "Data in Dictionary format. Let's turn it into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef2348e1-ca91-4862-a648-847b5158ab5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>total_lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>to investigate the efficacy of @ weeks of dail...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>a total of @ patients with primary knee oa wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>outcome measures included pain reduction and i...</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>pain was assessed using the visual analog pain...</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>secondary outcome measures included the wester...</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>serum levels of interleukin @ ( il-@ ) , il-@ ...</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>there was a clinically relevant reduction in t...</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>the mean difference between treatment arms ( @...</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>further , there was a clinically relevant redu...</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>these differences remained significant at @ we...</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>the outcome measures in rheumatology clinical ...</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CONCLUSIONS</td>\n",
       "      <td>low-dose oral prednisolone had both a short-te...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td>emotional eating is associated with overeating...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td>yet , empirical evidence for individual ( trai...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>the aim of this study was to test if attention...</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         target                                               text  \\\n",
       "0     OBJECTIVE  to investigate the efficacy of @ weeks of dail...   \n",
       "1       METHODS  a total of @ patients with primary knee oa wer...   \n",
       "2       METHODS  outcome measures included pain reduction and i...   \n",
       "3       METHODS  pain was assessed using the visual analog pain...   \n",
       "4       METHODS  secondary outcome measures included the wester...   \n",
       "5       METHODS  serum levels of interleukin @ ( il-@ ) , il-@ ...   \n",
       "6       RESULTS  there was a clinically relevant reduction in t...   \n",
       "7       RESULTS  the mean difference between treatment arms ( @...   \n",
       "8       RESULTS  further , there was a clinically relevant redu...   \n",
       "9       RESULTS  these differences remained significant at @ we...   \n",
       "10      RESULTS  the outcome measures in rheumatology clinical ...   \n",
       "11  CONCLUSIONS  low-dose oral prednisolone had both a short-te...   \n",
       "12   BACKGROUND  emotional eating is associated with overeating...   \n",
       "13   BACKGROUND  yet , empirical evidence for individual ( trai...   \n",
       "14    OBJECTIVE  the aim of this study was to test if attention...   \n",
       "\n",
       "    line_number  total_lines  \n",
       "0             0           11  \n",
       "1             1           11  \n",
       "2             2           11  \n",
       "3             3           11  \n",
       "4             4           11  \n",
       "5             5           11  \n",
       "6             6           11  \n",
       "7             7           11  \n",
       "8             8           11  \n",
       "9             9           11  \n",
       "10           10           11  \n",
       "11           11           11  \n",
       "12            0           10  \n",
       "13            1           10  \n",
       "14            2           10  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.DataFrame(train_samples)\n",
    "val_df = pd.DataFrame(val_samples)\n",
    "test_df = pd.DataFrame(test_samples)\n",
    "train_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cfaeb52-471e-448a-8a17-f70003e78492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "METHODS        59353\n",
       "RESULTS        57953\n",
       "CONCLUSIONS    27168\n",
       "BACKGROUND     21727\n",
       "OBJECTIVE      13839\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribution of labels in training data\n",
    "train_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "130989cc-03a4-41d3-a291-1c0bde300d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Length of differente lines\n",
    "train_df.total_lines.plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6f204f-cfe3-4a5a-8f65-f5ecbfd3d13c",
   "metadata": {},
   "source": [
    "### Get a list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81a6df0f-656b-4a2a-bf8c-5d6b79d820b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180040, 30212, 30135)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert abstract text lines into lists\n",
    "train_sentences = train_df['text'].tolist()\n",
    "val_sentences = val_df['text'].tolist()\n",
    "test_sentences = test_df['text'].tolist()\n",
    "len(train_sentences), len(val_sentences), len(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27033844-b77f-49d3-8a64-064a66ba6bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .',\n",
       " 'a total of @ patients with primary knee oa were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .',\n",
       " 'outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .',\n",
       " 'pain was assessed using the visual analog pain scale ( @-@ mm ) .',\n",
       " 'secondary outcome measures included the western ontario and mcmaster universities osteoarthritis index scores , patient global assessment ( pga ) of the severity of knee oa , and @-min walk distance ( @mwd ) .',\n",
       " 'serum levels of interleukin @ ( il-@ ) , il-@ , tumor necrosis factor ( tnf ) - , and high-sensitivity c-reactive protein ( hscrp ) were measured .',\n",
       " 'there was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , pga , and @mwd at @ weeks .',\n",
       " 'the mean difference between treatment arms ( @ % ci ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 8 training sentences\n",
    "train_sentences[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d57232-117f-4fec-915d-4d96685044bc",
   "metadata": {},
   "source": [
    "## Labelling Target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e98185cb-ade8-4c72-8bf2-5e8312556077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encode labels\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=False) # tensorflow is incompatible with sparse matrix datatype\n",
    "train_labels_ohe = ohe.fit_transform(train_df['target'].to_numpy().reshape(-1,1))\n",
    "val_labels_ohe = ohe.transform(val_df['target'].to_numpy().reshape(-1,1))\n",
    "test_labels_ohe = ohe.transform(test_df['target'].to_numpy().reshape(-1,1))\n",
    "\n",
    "# checking training labels\n",
    "train_labels_ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01720b7e-1fdc-4afc-b64b-fa38c426ee15",
   "metadata": {},
   "source": [
    "### Label Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6032c6dd-116c-46f8-8dd4-0a9caae413f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 2, ..., 4, 1, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract labels ('target') and encode them into integers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_enc = label_encoder.fit_transform(train_df['target'].to_numpy())\n",
    "val_labels_enc = label_encoder.transform(val_df['target'].to_numpy())\n",
    "test_labels_enc = label_encoder.transform(test_df['target'].to_numpy())\n",
    "\n",
    "# checking training labels\n",
    "train_labels_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af1581d9-febb-441e-bcfa-134b37c0fb6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " array(['BACKGROUND', 'CONCLUSIONS', 'METHODS', 'OBJECTIVE', 'RESULTS'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting class names and number of classes from LabelEncoder instance\n",
    "num_classes = len(label_encoder.classes_)\n",
    "class_names = label_encoder.classes_\n",
    "num_classes, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ea2a8a-157a-4096-87bd-199f8e2e1e0c",
   "metadata": {},
   "source": [
    "## Creating a Series of Model Experiments\n",
    "\n",
    "Trying a few different models and watching their behaviour.\n",
    "\n",
    "> Tip: [Chris Albon](https://chrisalbon.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80840e4-d113-4ace-a9ee-72085b7aa41d",
   "metadata": {},
   "source": [
    "## Model 0: Getting a baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54859e17-5f54-4b98-b711-b02ef37ccc02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tf-idf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF multinomial Naive Bayes model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Creating a pipeline\n",
    "## usually multinomialnb accepts label encoding\n",
    "model_0 = Pipeline([\n",
    "    ('tf-idf', TfidfVectorizer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_0.fit(X=train_sentences, y=train_labels_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6db14b54-92ed-436b-9be9-ca5f47934606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7218323844829869"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate baseline on validation dataset\n",
    "model_0.score(X=val_sentences, y=val_labels_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c38a9ebe-82f2-47af-b96f-1b90cdc802be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 1, 3, ..., 4, 4, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making Predictions\n",
    "baseline_preds = model_0.predict(val_sentences)\n",
    "baseline_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ee5e8d1-9a3d-4771-819a-2d2610965148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 72.1832384482987,\n",
       " 'precision': 0.7186466952323352,\n",
       " 'recall': 0.7218323844829869,\n",
       " 'f1': 0.6989250353450294}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting baseline results\n",
    "baseline_results = help_me.calculate_results(val_labels_enc, baseline_preds)\n",
    "baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bcbdec-83ef-44b6-bda9-3aed9def9d35",
   "metadata": {},
   "source": [
    "### Preparing the data (text) for deep sequence models\n",
    "\n",
    "`vectorization` and `embedding_layers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cffb44e-4be2-4590-86d2-f8555378d1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.338269273494777"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# measuring sentence length\n",
    "sent_len = [len(sentence.split()) for sentence in train_sentences]\n",
    "avg_sent_len = np.mean(sent_len)\n",
    "avg_sent_len # average sentenc length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e167444-c408-4a0c-9b1d-39280160b9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evualitng distribution of sentences' lengths\n",
    "plt.hist(sent_len, bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23200ced-8d86-4bea-b5b5-176aab19ce30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% of the sentences has until 55 tokens - Max length: 296\n"
     ]
    }
   ],
   "source": [
    "# where do the 95% of the lengths reaches?\n",
    "output_seq_len = int(np.percentile(sent_len, 95))\n",
    "\n",
    "# max sentence\n",
    "max_sent = max(sent_len)\n",
    "print(f'95% of the sentences has until {output_seq_len} tokens - Max length: {max_sent}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1504b3-3d3f-4a7e-a20f-c251a29a6326",
   "metadata": {},
   "source": [
    "### Text Vectorizer Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3f84c64-b945-4f64-8af6-ae6475b74a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# according to the paper, there are ~68k tokens\n",
    "max_tokens = 68000\n",
    "\n",
    "# Creating vectorizer\n",
    "text_vectorizer = layers.TextVectorization(max_tokens=max_tokens, output_sequence_length=55)\n",
    "\n",
    "# adapting text vectorizer to training sentences\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2e6ea6b-156e-4793-90d7-950564a336c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "the appearance of each half of the scar was assessed at @ week , @ weeks , and @ months by the principal investigator .\n",
      "\n",
      "Length of the Text: 25\n",
      "\n",
      "Vectorized text: [[   2 3063    4  122 1398    4    2 1912   10  113   15   89   53    3\n",
      "    41   22    2 3528 3176    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "# test out text vectorizer\n",
    "rand_sentence = random.choice(train_sentences)\n",
    "print(f'Text:\\n{rand_sentence}')\n",
    "print(f'\\nLength of the Text: {len(rand_sentence.split())}')\n",
    "print(f'\\nVectorized text: {text_vectorizer([rand_sentence])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e81aa78c-a61b-4867-a8b1-75bdbb6f46b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocabulary: 64841\n",
      "Most common words in the vocabulary: ['', '[UNK]', 'the', 'and', 'of']\n",
      "Least common words in the vocabulary: ['aainduced', 'aaigroup', 'aachener', 'aachen', 'aaacp']\n"
     ]
    }
   ],
   "source": [
    "# checking more informations on training data\n",
    "rct_20k_text_vocab = text_vectorizer.get_vocabulary()\n",
    "print(f'Number of words in vocabulary: {len(rct_20k_text_vocab)}')\n",
    "print(f'Most common words in the vocabulary: {rct_20k_text_vocab[:5]}')\n",
    "print(f'Least common words in the vocabulary: {rct_20k_text_vocab[-5:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "965edd4a-ca37-4ef2-a869-10500b89e87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'text_vectorization',\n",
       " 'trainable': True,\n",
       " 'batch_input_shape': (None,),\n",
       " 'dtype': 'string',\n",
       " 'max_tokens': 68000,\n",
       " 'standardize': 'lower_and_strip_punctuation',\n",
       " 'split': 'whitespace',\n",
       " 'ngrams': None,\n",
       " 'output_mode': 'int',\n",
       " 'output_sequence_length': 55,\n",
       " 'pad_to_max_tokens': False,\n",
       " 'sparse': False,\n",
       " 'ragged': False,\n",
       " 'vocabulary': None,\n",
       " 'idf_weights': None}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# general configurations of the vectorizer\n",
    "text_vectorizer.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18ef269-aa33-42a7-85d3-f5bdcfdb0be8",
   "metadata": {},
   "source": [
    "### Creating Text Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "665c2532-54b7-4bf0-9f0e-c1ccce3032b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence before vectorization:\n",
      "the appearance of each half of the scar was assessed at @ week , @ weeks , and @ months by the principal investigator .\n",
      "\n",
      "Sentence after vectorization (before embedding):\n",
      "[   2 3063    4  122 1398    4    2 1912   10  113   15   89   53    3\n",
      "   41   22    2 3528 3176    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "Sentence after embedding: [[ 0.02303988 -0.03269496  0.0195527  ...  0.02661157 -0.02259035\n",
      "  -0.01640763]\n",
      " [ 0.00204987  0.02991429 -0.02266381 ...  0.01789511 -0.00383307\n",
      "  -0.0492411 ]\n",
      " [-0.00220469  0.00545921 -0.02557361 ... -0.01321427 -0.03368282\n",
      "  -0.01106764]\n",
      " ...\n",
      " [ 0.01964394 -0.00804216 -0.00928305 ...  0.04644332 -0.02247282\n",
      "  -0.01954718]\n",
      " [ 0.01964394 -0.00804216 -0.00928305 ...  0.04644332 -0.02247282\n",
      "  -0.01954718]\n",
      " [ 0.01964394 -0.00804216 -0.00928305 ...  0.04644332 -0.02247282\n",
      "  -0.01954718]]\n",
      "\n",
      "Embed sentence shape: (1, 55, 128)\n"
     ]
    }
   ],
   "source": [
    "# creating token embedding layer\n",
    "token_embed = layers.Embedding(input_dim = len(rct_20k_text_vocab), output_dim=128,\n",
    "                              mask_zero=True, name='token_embedding')\n",
    "\n",
    "# show example embedding\n",
    "print(f'Sentence before vectorization:\\n{rand_sentence}\\n')\n",
    "vec_sentence = text_vectorizer(rand_sentence)\n",
    "print(f'Sentence after vectorization (before embedding):\\n{vec_sentence}')\n",
    "embed_sentence = token_embed(vec_sentence)\n",
    "print(f'Sentence after embedding: {embed_sentence}\\n')\n",
    "print(f'Embed sentence shape: {np.expand_dims(embed_sentence,axis=0).shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9244df3-de3a-4de4-9b47-659871360153",
   "metadata": {},
   "source": [
    "## Creating Datasets\n",
    "\n",
    "It's possible to optimize the data loading. One possible method is to turn the data into\n",
    "`PrefetchDataset`. In this way, we cam prepare the data better. \n",
    "\n",
    "Read documentation:\n",
    " * [tf.data: Build TensorFlow input pipelines](https://www.tensorflow.org/guide/data)\n",
    " * [Better performance with the tf.data API](https://www.tensorflow.org/guide/data_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40600113-a5b4-4cc8-98a3-97f9187365fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(5,), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turning the data into tensorflow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_ohe))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_ohe))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels_ohe))\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c81235e9-9a39-48f3-81b5-104e449787c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking the TensorSliceDataset's and turn them into prefetched batches\n",
    "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(32).prefetch(32)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ae55d8-8b90-46c4-b650-93519870302c",
   "metadata": {},
   "source": [
    "## Model 1: Conv1D with Token Embeddings\n",
    "\n",
    "Sequence to be made:\n",
    "`Input (text) -> Tokenize -> Embedding -> Layers -> Output (label probability)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b80ad30-0fcb-46fd-9cbb-28ee2bf54344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 1D convolutional model to process sequences\n",
    "\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "text_vectors = text_vectorizer(inputs) # vectorize text inputs\n",
    "token_embeddings = token_embed(text_vectors) # create embedding\n",
    "x = layers.Conv1D(64, kernel_size = 5, padding='same', activation='relu')(token_embeddings)\n",
    "x = layers.GlobalAveragePooling1D()(x) # condense the output of our feature vector\n",
    "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "model_1 = tf.keras.Model(inputs, outputs, name='model_1')\n",
    "\n",
    "# compile the model\n",
    "model_1.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e20ffa16-49d8-4670-81bc-91e11c650c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization (TextVec  (None, 55)               0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " token_embedding (Embedding)  (None, 55, 128)          8299648   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 55, 64)            41024     \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 64)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,340,997\n",
      "Trainable params: 8,340,997\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# get summary\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1b5033dd-71ba-4500-b4e7-201d281c641f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1688/1688 [==============================] - 26s 12ms/step - loss: 0.7290 - accuracy: 0.7246 - val_loss: 0.5869 - val_accuracy: 0.7915\n",
      "Epoch 2/3\n",
      "1688/1688 [==============================] - 19s 11ms/step - loss: 0.5850 - accuracy: 0.7910 - val_loss: 0.5473 - val_accuracy: 0.8040\n",
      "Epoch 3/3\n",
      "1688/1688 [==============================] - 18s 11ms/step - loss: 0.5473 - accuracy: 0.8072 - val_loss: 0.5451 - val_accuracy: 0.8028\n"
     ]
    }
   ],
   "source": [
    "# fitting the data (for now only in 30% of the original data)\n",
    "## train and validate on only 30% of the data\n",
    "history_1 = model_1.fit(train_dataset, steps_per_epoch=int(0.3*len(train_dataset)), epochs=3,\n",
    "                       validation_data=val_dataset, validation_steps=int(0.3*len(val_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33434c2b-6de6-4efe-ac37-fc75cb63bbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 3s 3ms/step - loss: 0.5452 - accuracy: 0.8055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5452253818511963, 0.8055408596992493]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluating on the whole validation dataset\n",
    "model_1.evaluate(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "918d3acc-582b-4a6f-a43f-677bfcdfa021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.86134219e-01, 1.13256037e-01, 5.19569106e-02, 1.09810457e-01,\n",
       "        3.88423540e-02],\n",
       "       [6.31314397e-01, 2.17948362e-01, 1.16928965e-02, 1.25392228e-01,\n",
       "        1.36521477e-02],\n",
       "       [3.84117603e-01, 7.64553761e-03, 3.98825668e-03, 6.04151428e-01,\n",
       "        9.71276822e-05],\n",
       "       ...,\n",
       "       [4.18130548e-06, 1.18373086e-04, 1.93011481e-03, 2.53783924e-06,\n",
       "        9.97944772e-01],\n",
       "       [1.25864789e-01, 3.23156774e-01, 1.85972616e-01, 4.92084287e-02,\n",
       "        3.15797448e-01],\n",
       "       [1.04042083e-01, 8.52155268e-01, 3.61312740e-02, 2.03188485e-03,\n",
       "        5.63949998e-03]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making predictions (the model outputs probabilities for each class)\n",
    "model_1_pred_probs = model_1.predict(val_dataset)\n",
    "model_1_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b63fda71-3050-4e39-ba5e-ddbe2cbc4fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 0, 3, ..., 4, 1, 1], dtype=int64)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting pred probabilities to classes\n",
    "model_1_pred = tf.argmax(model_1_pred_probs, axis=1)\n",
    "model_1_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c4aa1c8-8016-433c-a947-cba26772a898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 80.55408446974712,\n",
       " 'precision': 0.8102288207357515,\n",
       " 'recall': 0.8055408446974712,\n",
       " 'f1': 0.7993683353268938}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_1 results\n",
    "model_1_results = help_me.calculate_results(y_true=val_labels_enc, y_pred=model_1_pred)\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c60e1-471d-49d8-bbe6-a926b55806ef",
   "metadata": {},
   "source": [
    "## Model 2: Feature Extraction w/ Pretrained Token Embeddings\n",
    "\n",
    "Using a pretrained Universal Sequence Encoder (USE) to initialize the token embeddings (pretrained embedding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "78f2f4d3-1f82-4d10-a236-454ec0118d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pretrained tensorflow hub USE\n",
    "tf_hub_emb_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                 trainable=False,\n",
    "                                 name='universal_sentence_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3868c3d6-3cf5-4668-9ad7-a9fb349dfbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Sentence:\n",
      " community-based research centers .\n",
      "Sentence after embedding: [ 0.04999234  0.02240906  0.00020972 -0.01061607 -0.0626808  -0.08024239\n",
      "  0.07699092 -0.06257688 -0.07751091 -0.05798236  0.03175253  0.03715825\n",
      "  0.07512141 -0.08810013  0.0315895  -0.00051465 -0.07209132  0.04926718\n",
      " -0.03594328 -0.07392235  0.00494869 -0.0565359  -0.00270954  0.00155925\n",
      "  0.01222181 -0.0133688  -0.01169132 -0.01577508 -0.01279055  0.0836598 ] (truncated output to 30).\n",
      "Length of sentence embedding: 512\n"
     ]
    }
   ],
   "source": [
    "# testing out the embedding on a random sentence\n",
    "print(f'Random Sentence:\\n {rand_sentence}')\n",
    "embed_seq = tf_hub_emb_layer([rand_sentence])\n",
    "print(f'Sentence after embedding: {embed_seq[0][:30]} (truncated output to 30).')\n",
    "print(f'Length of sentence embedding: {len(embed_seq[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1d47220a-01bb-49d4-b69b-74a942041dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b579ee86-af37-42ef-a740-b5b16d404850",
   "metadata": {},
   "source": [
    "### Building and Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4b114305-c8ac-4c3c-ac64-59cc08c884a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text, labels in train_dataset.take(1):  # only take first element of dataset\n",
    "#     numpy_text = text.numpy()\n",
    "#     numpy_labels = labels.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f08f2411-63c1-4fe4-ad72-9afe50275481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extractor model with TF Hub Layer (USE)\n",
    "\n",
    "inputs = layers.Input(shape=[], dtype=tf.string) # inputs of USE are lists\n",
    "embedded_layer = tf_hub_emb_layer(inputs)\n",
    "x = layers.Dense(128, activation='relu')(embedded_layer)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "outputs = layers.Dense(len(class_names), activation='softmax')(x)\n",
    "model_2 = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# compiling the model\n",
    "model_2.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy']) #values are OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "89093a18-8b62-4695-ae40-fc46c1597fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None,)]                 0         \n",
      "                                                                 \n",
      " universal_sentence_encoder   (None, 512)              256797824 \n",
      " (KerasLayer)                                                    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256,872,069\n",
      "Trainable params: 74,245\n",
      "Non-trainable params: 256,797,824\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fa05326c-1f94-4b11-a375-55dd1b637b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "5627/5627 [==============================] - 100s 17ms/step - loss: 0.6970 - accuracy: 0.7326 - val_loss: 0.6326 - val_accuracy: 0.7572\n",
      "Epoch 2/3\n",
      "5627/5627 [==============================] - 94s 17ms/step - loss: 0.6087 - accuracy: 0.7672 - val_loss: 0.6048 - val_accuracy: 0.7690\n",
      "Epoch 3/3\n",
      "5627/5627 [==============================] - 94s 17ms/step - loss: 0.5754 - accuracy: 0.7812 - val_loss: 0.6000 - val_accuracy: 0.7706\n"
     ]
    }
   ],
   "source": [
    "history_2 = model_2.fit(train_dataset, epochs=3, steps_per_epoch=len(train_dataset), validation_data=val_dataset,\n",
    "                       validation_steps=len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3abd3f30-db1e-4ec4-91cc-1b6976fa821e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "942/942 [==============================] - 14s 15ms/step - loss: 0.6225 - accuracy: 0.7649\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.622536301612854, 0.7648581266403198]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate on test dataset\n",
    "model_2.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "94e39a49-945c-4f32-a565-5b57df3de3a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.21065030e-01, 1.40405446e-01, 1.62753463e-01, 1.43608317e-01,\n",
       "        3.32167745e-01],\n",
       "       [1.27343088e-01, 8.69997125e-03, 6.06662095e-01, 1.83495745e-01,\n",
       "        7.37990290e-02],\n",
       "       [1.84474178e-04, 7.07519997e-04, 9.60877895e-01, 4.07941756e-04,\n",
       "        3.78222615e-02],\n",
       "       ...,\n",
       "       [6.14856663e-06, 6.80315401e-03, 4.95160697e-04, 1.61401556e-06,\n",
       "        9.92693901e-01],\n",
       "       [7.35218986e-04, 2.67261565e-02, 2.37505697e-03, 1.07065214e-04,\n",
       "        9.70056474e-01],\n",
       "       [1.03722354e-02, 7.79365659e-01, 7.32115731e-02, 1.56648252e-02,\n",
       "        1.21385790e-01]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting probabilities\n",
    "model_2_pred_probs = model_2.predict(test_dataset)\n",
    "model_2_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3e36f6ca-82db-4d4c-9578-8ea6a2adf8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30135,), dtype=int64, numpy=array([4, 2, 2, ..., 4, 4, 1], dtype=int64)>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting the predictions to classes\n",
    "model_2_pred = tf.argmax(model_2_pred_probs, axis=1) # return the index of the highest value | axis = 1 compares columns\n",
    "model_2_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b6822ba3-8685-49a7-b167-d0ef95705ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 76.4858138377302,\n",
       " 'precision': 0.7611258860613508,\n",
       " 'recall': 0.7648581383773021,\n",
       " 'f1': 0.7607414389715157}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the result of the metrics with the test dataset\n",
    "model_2_results = help_me.calculate_results(y_true=test_labels_enc, y_pred=model_2_pred)\n",
    "model_2_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fb9640-6a44-402c-9916-7c8d4d8efcb5",
   "metadata": {},
   "source": [
    "## Model 3: Conv1D with Character Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9a3744-d49f-4f1a-a741-66f99cf04311",
   "metadata": {},
   "source": [
    "### Creating a char-level tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "db5c1c4f-b239-493c-b139-c8dc0d3cb23f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t h e   a p p e a r a n c e   o f   e a c h   h a l f   o f   t h e   s c a r   w a s   a s s e s s e d   a t   @   w e e k   ,   @   w e e k s   ,   a n d   @   m o n t h s   b y   t h e   p r i n c i p a l   i n v e s t i g a t o r   .'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building function to help extract character from sentences\n",
    "def split_sentences(sentences):\n",
    "    return ' '.join(list(sentences))\n",
    "\n",
    "# splitting non-character-level sequence into sentences\n",
    "split_sentences(rand_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c8d73617-15c9-46a8-ae5e-a6cdabdc876f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t o   i n v e s t i g a t e   t h e   e f f i c a c y   o f   @   w e e k s   o f   d a i l y   l o w - d o s e   o r a l   p r e d n i s o l o n e   i n   i m p r o v i n g   p a i n   ,   m o b i l i t y   ,   a n d   s y s t e m i c   l o w - g r a d e   i n f l a m m a t i o n   i n   t h e   s h o r t   t e r m   a n d   w h e t h e r   t h e   e f f e c t   w o u l d   b e   s u s t a i n e d   a t   @   w e e k s   i n   o l d e r   a d u l t s   w i t h   m o d e r a t e   t o   s e v e r e   k n e e   o s t e o a r t h r i t i s   (   o a   )   .'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating data-level character splittings\n",
    "train_char = [split_sentences(sentence) for sentence in train_sentences]\n",
    "val_char = [split_sentences(sentence) for sentence in val_sentences]\n",
    "test_char = [split_sentences(sentence) for sentence in test_sentences]\n",
    "\n",
    "train_char[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40851fbb-faf9-41ef-9882-40cf8472c446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149.3662574983337"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking average character length\n",
    "char_len = [len(sentence) for sentence in train_sentences]\n",
    "mean_char_len = np.mean(char_len)\n",
    "mean_char_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a58400e-a164-4427-b7a3-5891b96216a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD4CAYAAADsKpHdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAREUlEQVR4nO3df6zddX3H8edrrSLoUH4Uhi3ZrbNxA5INaRjqYpbVjCrG8ocmXeboti5NDNvUbXFlJjP7gwQ2I0o2WQg4CjqxqW40GjZJ0SxLWNnFX/yojCoIlQp1IDIX0ep7f5z3naeX29tzy/1xsM9HcnK+5/39fr7n/b25977u9/M959xUFZIk/cxSNyBJGg8GgiQJMBAkSc1AkCQBBoIkqS1f6gaO1qmnnloTExNL3YYkPa/cdddd366qFTOte94GwsTEBJOTk0vdhiQ9ryT5xuHWOWUkSQIMBElSMxAkSYCBIElqBoIkCTAQJEnNQJAkAQaCJKkZCJIk4Hn8TuXnYmLrZ5bsuR+64qIle25Jmo1nCJIkwECQJDUDQZIEGAiSpGYgSJIAA0GS1AwESRJgIEiSmoEgSQIMBElSMxAkSYCBIElqBoIkCTAQJEnNQJAkAQaCJKkZCJIkwECQJDUDQZIEGAiSpGYgSJIAA0GS1AwESRJgIEiS2kiBkOTdSe5Nck+Sjyd5UZKTk9yW5IG+P2lo+8uS7E1yf5ILh+rnJbm7112dJF0/Lsknur47ycS8H6kkaVZHDIQkK4E/BtZW1TnAMmAjsBXYVVVrgF39mCRn9fqzgfXAh5Ms691dA2wB1vRtfdc3A09W1SuBq4Ar5+XoJEkjG3XKaDlwfJLlwAnAo8AGYFuv3wZc3MsbgJur6pmqehDYC5yf5AzgxKq6o6oKuHHamKl97QDWTZ09SJIWxxEDoaq+CbwfeBjYDzxVVZ8FTq+q/b3NfuC0HrISeGRoF/u6trKXp9cPGVNVB4GngFOm95JkS5LJJJMHDhwY9RglSSMYZcroJAZ/wa8GXg68OMnbZxsyQ61mqc825tBC1bVVtbaq1q5YsWL2xiVJczLKlNEbgAer6kBV/RD4FPBa4LGeBqLvH+/t9wFnDo1fxWCKaV8vT68fMqanpV4KPHE0ByRJOjqjBMLDwAVJTuh5/XXAHmAnsKm32QTc0ss7gY39yqHVDC4e39nTSk8nuaD3c8m0MVP7eitwe19nkCQtkuVH2qCqdifZAXwBOAh8EbgWeAmwPclmBqHxtt7+3iTbgft6+0ur6ke9u3cANwDHA7f2DeB64KYkexmcGWycl6OTJI3siIEAUFXvA943rfwMg7OFmba/HLh8hvokcM4M9e/TgSJJWhq+U1mSBBgIkqQ20pSR5s/E1s8syfM+dMVFS/K8kp4/PEOQJAEGgiSpGQiSJMBAkCQ1A0GSBBgIkqRmIEiSAANBktQMBEkSYCBIkpqBIEkCDARJUjMQJEmAgSBJagaCJAkwECRJzUCQJAEGgiSpGQiSJMBAkCQ1A0GSBBgIkqRmIEiSAANBktQMBEkSYCBIkpqBIEkCDARJUjMQJEmAgSBJagaCJAkwECRJzUCQJAEjBkKSlyXZkeSrSfYkeU2Sk5PcluSBvj9paPvLkuxNcn+SC4fq5yW5u9ddnSRdPy7JJ7q+O8nEvB+pJGlWo54hfAj4l6r6ReCXgT3AVmBXVa0BdvVjkpwFbATOBtYDH06yrPdzDbAFWNO39V3fDDxZVa8ErgKufI7HJUmaoyMGQpITgdcD1wNU1Q+q6jvABmBbb7YNuLiXNwA3V9UzVfUgsBc4P8kZwIlVdUdVFXDjtDFT+9oBrJs6e5AkLY5RzhBeARwA/iHJF5Ncl+TFwOlVtR+g70/r7VcCjwyN39e1lb08vX7ImKo6CDwFnDK9kSRbkkwmmTxw4MCIhyhJGsUogbAceDVwTVWdC3yPnh46jJn+sq9Z6rONObRQdW1Vra2qtStWrJi9a0nSnIwSCPuAfVW1ux/vYBAQj/U0EH3/+ND2Zw6NXwU82vVVM9QPGZNkOfBS4Im5Howk6egdMRCq6lvAI0le1aV1wH3ATmBT1zYBt/TyTmBjv3JoNYOLx3f2tNLTSS7o6wOXTBszta+3Arf3dQZJ0iJZPuJ2fwR8LMkLga8Dv8cgTLYn2Qw8DLwNoKruTbKdQWgcBC6tqh/1ft4B3AAcD9zaNxhcsL4pyV4GZwYbn+NxSZLmaKRAqKovAWtnWLXuMNtfDlw+Q30SOGeG+vfpQJEkLQ3fqSxJAgwESVIzECRJgIEgSWoGgiQJMBAkSc1AkCQBBoIkqRkIkiTAQJAkNQNBkgQYCJKkZiBIkgADQZLUDARJEmAgSJKagSBJAgwESVIzECRJgIEgSWoGgiQJMBAkSc1AkCQBBoIkqRkIkiTAQJAkNQNBkgQYCJKkZiBIkgADQZLUDARJEmAgSJKagSBJAgwESVIzECRJwBwCIcmyJF9M8ul+fHKS25I80PcnDW17WZK9Se5PcuFQ/bwkd/e6q5Ok68cl+UTXdyeZmMdjlCSNYC5nCO8E9gw93grsqqo1wK5+TJKzgI3A2cB64MNJlvWYa4AtwJq+re/6ZuDJqnolcBVw5VEdjSTpqI0UCElWARcB1w2VNwDbenkbcPFQ/eaqeqaqHgT2AucnOQM4saruqKoCbpw2ZmpfO4B1U2cPkqTFMeoZwgeB9wA/HqqdXlX7Afr+tK6vBB4Z2m5f11b28vT6IWOq6iDwFHDK9CaSbEkymWTywIEDI7YuSRrFEQMhyZuBx6vqrhH3OdNf9jVLfbYxhxaqrq2qtVW1dsWKFSO2I0kaxfIRtnkd8JYkbwJeBJyY5KPAY0nOqKr9PR30eG+/DzhzaPwq4NGur5qhPjxmX5LlwEuBJ47ymCRJR+GIZwhVdVlVraqqCQYXi2+vqrcDO4FNvdkm4JZe3gls7FcOrWZw8fjOnlZ6OskFfX3gkmljpvb11n6OZ50hSJIWzihnCIdzBbA9yWbgYeBtAFV1b5LtwH3AQeDSqvpRj3kHcANwPHBr3wCuB25KspfBmcHG59CXJOkozCkQqurzwOd7+b+BdYfZ7nLg8hnqk8A5M9S/TweKJGlp+E5lSRJgIEiSmoEgSQIMBElSMxAkSYCBIElqBoIkCTAQJEnNQJAkAQaCJKkZCJIkwECQJDUDQZIEGAiSpGYgSJIAA0GS1AwESRJgIEiSmoEgSQIMBElSMxAkSYCBIElqBoIkCTAQJEnNQJAkAQaCJKkZCJIkwECQJDUDQZIEGAiSpGYgSJIAA0GS1AwESRJgIEiSmoEgSQIMBElSO2IgJDkzyeeS7Elyb5J3dv3kJLcleaDvTxoac1mSvUnuT3LhUP28JHf3uquTpOvHJflE13cnmViAY5UkzWKUM4SDwJ9W1S8BFwCXJjkL2Arsqqo1wK5+TK/bCJwNrAc+nGRZ7+saYAuwpm/ru74ZeLKqXglcBVw5D8cmSZqDIwZCVe2vqi/08tPAHmAlsAHY1pttAy7u5Q3AzVX1TFU9COwFzk9yBnBiVd1RVQXcOG3M1L52AOumzh4kSYtjTtcQeirnXGA3cHpV7YdBaACn9WYrgUeGhu3r2spenl4/ZExVHQSeAk6Z4fm3JJlMMnngwIG5tC5JOoKRAyHJS4BPAu+qqu/OtukMtZqlPtuYQwtV11bV2qpau2LFiiO1LEmag5ECIckLGITBx6rqU11+rKeB6PvHu74POHNo+Crg0a6vmqF+yJgky4GXAk/M9WAkSUdvlFcZBbge2FNVHxhatRPY1MubgFuG6hv7lUOrGVw8vrOnlZ5OckHv85JpY6b29Vbg9r7OIElaJMtH2OZ1wO8Adyf5Utf+ArgC2J5kM/Aw8DaAqro3yXbgPgavULq0qn7U494B3AAcD9zaNxgEzk1J9jI4M9j43A5LkjRXRwyEqvp3Zp7jB1h3mDGXA5fPUJ8Ezpmh/n06UCRJS8N3KkuSAANBktQMBEkSYCBIkpqBIEkCDARJUjMQJEmAgSBJagaCJAkwECRJzUCQJAEGgiSpGQiSJMBAkCQ1A0GSBBgIkqRmIEiSgNH+haZ+Ckxs/cySPfdDV1y0ZM8taXSeIUiSAANBktQMBEkSYCBIkpqBIEkCDARJUjMQJEmAgSBJagaCJAkwECRJzUCQJAEGgiSpGQiSJMBAkCQ1A0GSBBgIkqRmIEiSAANBktTG5l9oJlkPfAhYBlxXVVcscUuaJ0v17zv9153S3IzFGUKSZcDfAW8EzgJ+K8lZS9uVJB1bxuUM4Xxgb1V9HSDJzcAG4L4l7UrPa56ZSHMzLoGwEnhk6PE+4Fenb5RkC7ClH/5PkvuP8vlOBb59lGOXgv0urHntN1fO154O65j++i6Cn/Z+f/5wK8YlEDJDrZ5VqLoWuPY5P1kyWVVrn+t+Fov9Liz7XVj2u7Dms9+xuIbA4IzgzKHHq4BHl6gXSTomjUsg/CewJsnqJC8ENgI7l7gnSTqmjMWUUVUdTPKHwL8yeNnpR6rq3gV8yuc87bTI7Hdh2e/Cst+FNW/9pupZU/WSpGPQuEwZSZKWmIEgSQKOwUBIsj7J/Un2Jtk6Bv2cmeRzSfYkuTfJO7t+cpLbkjzQ9ycNjbms+78/yYVL1PeyJF9M8ulx7zfJy5LsSPLV/jq/Zsz7fXd/L9yT5ONJXjRO/Sb5SJLHk9wzVJtzf0nOS3J3r7s6yUwvP1+ofv+mvx++kuSfkrxsnPsdWvdnSSrJqQvSb1UdMzcGF6y/BrwCeCHwZeCsJe7pDODVvfyzwH8x+PiOvwa2dn0rcGUvn9V9Hwes7uNZtgR9/wnwj8Cn+/HY9gtsA/6gl18IvGxc+2XwJs0HgeP78Xbgd8epX+D1wKuBe4Zqc+4PuBN4DYP3Id0KvHER+/1NYHkvXznu/Xb9TAYvvPkGcOpC9HusnSH8/0dkVNUPgKmPyFgyVbW/qr7Qy08Dexj8UtjA4BcZfX9xL28Abq6qZ6rqQWAvg+NaNElWARcB1w2Vx7LfJCcy+AG7HqCqflBV3xnXftty4Pgky4ETGLwnZ2z6rap/A56YVp5Tf0nOAE6sqjtq8NvrxqExC95vVX22qg72w/9g8N6nse23XQW8h0PftDuv/R5rgTDTR2SsXKJeniXJBHAusBs4var2wyA0gNN6s3E4hg8y+Mb88VBtXPt9BXAA+Iee4rouyYvHtd+q+ibwfuBhYD/wVFV9dlz7HTLX/lb28vT6Uvh9Bn9Bw5j2m+QtwDer6svTVs1rv8daIIz0ERlLIclLgE8C76qq78626Qy1RTuGJG8GHq+qu0YdMkNtMb/myxmcfl9TVecC32MwpXE4S/31PYnBX32rgZcDL07y9tmGzFAbi+/pdrj+xqLvJO8FDgIfmyrNsNmS9pvkBOC9wF/OtHqG2lH3e6wFwlh+REaSFzAIg49V1ae6/Fif9tH3j3d9qY/hdcBbkjzEYMrtN5J8lPHtdx+wr6p29+MdDAJiXPt9A/BgVR2oqh8CnwJeO8b9Tplrf/v4yTTNcH3RJNkEvBn47Z5WgfHs9xcY/IHw5f65WwV8IcnPMc/9HmuBMHYfkdFX/q8H9lTVB4ZW7QQ29fIm4Jah+sYkxyVZDaxhcPFoUVTVZVW1qqomGHz9bq+qt49xv98CHknyqi6tY/Cx6mPZL4OpoguSnNDfG+sYXFca136nzKm/nlZ6OskFfZyXDI1ZcBn8Q64/B95SVf87tGrs+q2qu6vqtKqa6J+7fQxeiPKtee93Ia6Sj/MNeBODV/J8DXjvGPTzawxO5b4CfKlvbwJOAXYBD/T9yUNj3tv9388CvdJhxN5/nZ+8ymhs+wV+BZjsr/E/AyeNeb9/BXwVuAe4icErSMamX+DjDK5v/LB/OW0+mv6AtX2MXwP+lv7khEXqdy+Dufepn7m/H+d+p61/iH6V0Xz360dXSJKAY2/KSJJ0GAaCJAkwECRJzUCQJAEGgiSpGQiSJMBAkCS1/wOyaRnc1O0kMQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# checking distribuiton of char-level lengths\n",
    "plt.hist(char_len, bins=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dafb541e-5a64-40d2-9e16-da49b7e57bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking the third percentil corresponding to 95% of char lengths\n",
    "output_seq_char_len = int(np.percentile(char_len, 95))\n",
    "output_seq_char_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a0de0544-8bbc-446a-900d-84d808e7b8cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefghijklmnopqrstuvwxyz0123456789!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numbers of tokens for text vectorizer\n",
    "alphabet = string.ascii_lowercase + string.digits + string.punctuation\n",
    "alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2a478004-bc4e-4846-acaa-3a528768fe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# char-level token vectorizer instance\n",
    "NUM_CHARS_TOKEN = len(alphabet) + 2 # OOV terms and ' ' (space)\n",
    "char_vectorizer = layers.TextVectorization(max_tokens=NUM_CHARS_TOKEN, output_sequence_length=output_seq_char_len,\n",
    "                                          standardize='lower_and_strip_punctuation', name='char_vectorizer')\n",
    "\n",
    "# adapt char vectorizer to training characters\n",
    "char_vectorizer.adapt(train_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "592be220-40f1-4076-b384-662cb76e8194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different characters in char_vocab: 28\n",
      "5 most common words in char_vocab: ['', '[UNK]', 'e', 't', 'i']\n",
      "5 least common words in char_vocab: ['k', 'x', 'z', 'q', 'j']\n"
     ]
    }
   ],
   "source": [
    "# checking char vocabulary characteristics\n",
    "char_vocab = char_vectorizer.get_vocabulary()\n",
    "print(f'Number of different characters in char_vocab: {len(char_vocab)}')\n",
    "print(f'5 most common words in char_vocab: {char_vocab[:5]}')\n",
    "print(f'5 least common words in char_vocab: {char_vocab[-5:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4defdc4d-03cc-427a-808a-d25359841390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char text: \n",
      "t h e r e   w e r e   n o   s i g n i f i c a n t   d i f f e r e n c e s   i n   t h e   c l i n i c a l   c h a r a c t e r i s t i c s   b e t w e e n   t h e   g r o u p s   a t   b a s e l i n e   .\n",
      "\n",
      "Char text length: \n",
      "88\n",
      "\n",
      "Vectorizer Chars: \n",
      "[ 3 13  2  8  2 20  2  8  2  6  7  9  4 18  6  4 17  4 11  5  6  3 10  4\n",
      " 17 17  2  8  2  6 11  2  9  4  6  3 13  2 11 12  4  6  4 11  5 12 11 13\n",
      "  5  8  5 11  3  2  8  4  9  3  4 11  9 22  2  3 20  2  2  6  3 13  2 18\n",
      "  8  7 16 14  9  5  3 22  5  9  2 12  4  6  2  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0]\n",
      "\n",
      "Length of Vectorized Chars: 290\n"
     ]
    }
   ],
   "source": [
    "# testing char vectorizer\n",
    "random_chars = random.choice(train_char)\n",
    "print(f'Char text: \\n{random_chars}')\n",
    "print(f'\\nChar text length: \\n{len(random_chars.split())}')\n",
    "print(f'\\nVectorizer Chars: \\n{char_vectorizer(random_chars)}')\n",
    "print(f'\\nLength of Vectorized Chars: {len(char_vectorizer(random_chars))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71eabc0-75c6-43b3-8ca8-55ef4dfa77e9",
   "metadata": {},
   "source": [
    "### Creating a char-level embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5b28d4b2-3d17-4112-bd98-5eb067acb72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating embedding layer\n",
    "emb_char_layer = layers.Embedding(input_dim=len(char_vocab), output_dim = 25, mask_zero=False, name='char_embed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f75a4100-1b28-4653-8d01-793b679d31c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char text:\n",
      "t h e r e   w e r e   n o   s i g n i f i c a n t   d i f f e r e n c e s   i n   t h e   c l i n i c a l   c h a r a c t e r i s t i c s   b e t w e e n   t h e   g r o u p s   a t   b a s e l i n e   .\n",
      "\n",
      "Embedded chars:\n",
      "[[[-0.01334344 -0.01115595 -0.04494194 ... -0.03887565  0.04782024\n",
      "   -0.01241448]\n",
      "  [-0.00413412  0.01929874  0.03192815 ... -0.03262569 -0.02945203\n",
      "   -0.0167611 ]\n",
      "  [-0.01876626  0.04941168  0.0060109  ...  0.0460773   0.0497751\n",
      "    0.03906249]\n",
      "  ...\n",
      "  [ 0.00715106  0.01826962  0.01032513 ...  0.04638679  0.03125017\n",
      "    0.01806043]\n",
      "  [ 0.00715106  0.01826962  0.01032513 ...  0.04638679  0.03125017\n",
      "    0.01806043]\n",
      "  [ 0.00715106  0.01826962  0.01032513 ...  0.04638679  0.03125017\n",
      "    0.01806043]]]\n",
      "Character embedding shape: (1, 290, 25)\n"
     ]
    }
   ],
   "source": [
    "# Testing embedded layer\n",
    "print(f'Char text:\\n{random_chars}\\n')\n",
    "print(f'Embedded chars:\\n{emb_char_layer(char_vectorizer([random_chars]))}')\n",
    "print(f'Character embedding shape: {emb_char_layer(char_vectorizer([random_chars])).shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1bf1edc6-efa2-40ba-9ee1-9628af9d5638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the model\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "vectors = char_vectorizer(inputs)\n",
    "embs = emb_char_layer(vectors)\n",
    "x = layers.Conv1D(128, kernel_size=9, activation='relu', padding='same')(embs)\n",
    "x = layers.Conv1D(64, kernel_size=5, activation='relu', padding='same')(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "model_3 = tf.keras.Model(inputs, outputs, name='model_3')\n",
    "\n",
    "# compiling the model\n",
    "model_3.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "53568da6-5958-4ff4-85c4-6a92f5ad47fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " char_vectorizer (TextVector  (None, 290)              0         \n",
      " ization)                                                        \n",
      "                                                                 \n",
      " char_embed (Embedding)      (None, 290, 25)           700       \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 290, 128)          28928     \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 290, 64)           41024     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 64)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 70,977\n",
      "Trainable params: 70,977\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model summary\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4f89fd3a-66dd-48f4-bb74-3fac13d2a30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating prefetched data in order to optimize data loading\n",
    "train_char_dataset = tf.data.Dataset.from_tensor_slices((train_char, train_labels_ohe)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_char_dataset = tf.data.Dataset.from_tensor_slices((val_char, val_labels_ohe)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_char_dataset = tf.data.Dataset.from_tensor_slices((test_char, test_labels_ohe)).batch(32).prefetch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "433d68d5-e3a1-4be6-84b8-e83874faf15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5627/5627 [==============================] - 90s 15ms/step - loss: 0.7798 - accuracy: 0.6986 - val_loss: 0.6682 - val_accuracy: 0.7506\n",
      "Epoch 2/5\n",
      "5627/5627 [==============================] - 74s 13ms/step - loss: 0.6078 - accuracy: 0.7730 - val_loss: 0.5969 - val_accuracy: 0.7807\n",
      "Epoch 3/5\n",
      "5627/5627 [==============================] - 561s 100ms/step - loss: 0.5589 - accuracy: 0.7932 - val_loss: 0.5730 - val_accuracy: 0.7880\n",
      "Epoch 4/5\n",
      "5627/5627 [==============================] - 57s 10ms/step - loss: 0.5279 - accuracy: 0.8046 - val_loss: 0.5609 - val_accuracy: 0.7935\n",
      "Epoch 5/5\n",
      "5627/5627 [==============================] - 56s 10ms/step - loss: 0.5048 - accuracy: 0.8133 - val_loss: 0.5698 - val_accuracy: 0.7911\n"
     ]
    }
   ],
   "source": [
    "# fitting the model with the data\n",
    "history_3 = model_3.fit(train_char_dataset, batch_size=32, epochs=5, steps_per_epoch=5627, validation_data=val_char_dataset,\n",
    "                        validation_steps=945)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8d499186-cbc1-4158-87f1-a569abf64bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "942/942 [==============================] - 5s 6ms/step - loss: 0.5977 - accuracy: 0.7817\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5977244973182678, 0.7817487716674805]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluating model\n",
    "model_3.evaluate(test_char_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "09cf64bf-576f-48de-a7ce-3e66b01fa1ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.6148900e-01, 1.6174294e-01, 5.1166829e-02, 1.7032297e-01,\n",
       "        3.5527819e-01],\n",
       "       [1.5462803e-02, 3.5096001e-02, 5.6635416e-01, 1.1218946e-02,\n",
       "        3.7186813e-01],\n",
       "       [1.7403161e-05, 6.5749096e-05, 8.8835776e-01, 1.6615511e-05,\n",
       "        1.1154251e-01],\n",
       "       ...,\n",
       "       [4.3190998e-06, 3.5978500e-03, 8.8440618e-03, 7.6254059e-06,\n",
       "        9.8754615e-01],\n",
       "       [2.7362604e-04, 3.6118224e-02, 5.6060846e-03, 2.6308489e-04,\n",
       "        9.5773900e-01],\n",
       "       [1.6426545e-04, 5.8388483e-02, 9.1813654e-03, 8.4607433e-05,\n",
       "        9.3218130e-01]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking model's probabilities\n",
    "model_3_pred_probs = model_3.predict(test_char_dataset)\n",
    "model_3_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "66a65c6f-d9b7-45cb-8282-90560f4a0671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30135,), dtype=int64, numpy=array([4, 2, 2, ..., 4, 4, 4], dtype=int64)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking model's categories from prediction prababilities\n",
    "model_3_preds = tf.argmax(model_3_pred_probs, axis=1)\n",
    "model_3_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "76c629c8-112b-45dd-941e-4cc816fa9e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 78.17487970798075,\n",
       " 'precision': 0.7797125174662126,\n",
       " 'recall': 0.7817487970798075,\n",
       " 'f1': 0.7780227200595281}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating model results\n",
    "model_3_results = help_me.calculate_results(y_true=test_labels_enc, y_pred=model_3_preds)\n",
    "model_3_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d14e32-bb05-4764-978b-eeb9528a3fda",
   "metadata": {},
   "source": [
    "## Model 4: Token Embeddings + Character Embeddings (Hybrid Embedding Layer)\n",
    "\n",
    "Steps to be made:\n",
    "1. Create a token-level model\n",
    "2. Create a character-level model\n",
    "3. Concatenate both\n",
    "4. Build a series of output layers on top of the previous one\n",
    "5. Construct a model which takes token and character-level sequences as input and produces sequence label probabilities as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2789d45d-ca0b-4ffb-a393-d1ddc6abe9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. token inputs/model\n",
    "token_input = layers.Input(shape=[], dtype=tf.string, name='token_input')\n",
    "token_embeddings = tf_hub_emb_layer(token_input)\n",
    "token_output = layers.Dense(128, activation='relu')(token_embeddings)\n",
    "token_model = tf.keras.Model(token_input, token_output, name='token_model')\n",
    "\n",
    "# 2. char inputs/model\n",
    "char_input = layers.Input(shape=(1,), dtype=tf.string, name='char_input')\n",
    "char_vector = char_vectorizer(char_input)\n",
    "char_embedds = emb_char_layer(char_vector)\n",
    "char_bid_lstm = layers.Bidirectional(layers.LSTM(25))(char_embedds) # 25x2 \n",
    "char_model = tf.keras.Model(char_input, char_bid_lstm, name='char_model')\n",
    "\n",
    "# 3. concatenating token and char inputs (hybrid token embedding)\n",
    "token_char_concat = layers.Concatenate(name='token_char_concat')([token_model.output, char_model.output])\n",
    "\n",
    "# 4. output layers\n",
    "combined_dropout = layers.Dropout(0.5)(token_char_concat)\n",
    "combined_Dense = layers.Dense(256, activation='relu')(combined_dropout)\n",
    "second_dropout = layers.Dropout(0.3)(combined_Dense)\n",
    "outputs = layers.Dense(num_classes, activation='softmax')(second_dropout)\n",
    "\n",
    "# 5. Building the model\n",
    "model_4 = tf.keras.Model([token_model.input, char_model.input], outputs, name='model_4_token_char_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5932314e-2ad4-44c9-9061-045587baa873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4_token_char_embeddings\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " char_input (InputLayer)        [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " token_input (InputLayer)       [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " char_vectorizer (TextVectoriza  (None, 290)         0           ['char_input[0][0]']             \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " universal_sentence_encoder (Ke  (None, 512)         256797824   ['token_input[0][0]']            \n",
      " rasLayer)                                                                                        \n",
      "                                                                                                  \n",
      " char_embed (Embedding)         (None, 290, 25)      700         ['char_vectorizer[1][0]']        \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 128)          65664       ['universal_sentence_encoder[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 50)           10200       ['char_embed[1][0]']             \n",
      "                                                                                                  \n",
      " token_char_concat (Concatenate  (None, 178)         0           ['dense_1[0][0]',                \n",
      " )                                                                'bidirectional[0][0]']          \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 178)          0           ['token_char_concat[0][0]']      \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 256)          45824       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 256)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 5)            1285        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 256,921,497\n",
      "Trainable params: 123,673\n",
      "Non-trainable params: 256,797,824\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model_4 summary\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "610c6a2f-66ee-48e5-8927-8db6b39457d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "# plotting the embedded model\n",
    "plot_model(model_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "49ceefe2-841a-4f84-af75-e17179a0c501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model_4.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb5326c-eff9-4c17-b455-8791eb90763b",
   "metadata": {},
   "source": [
    "### Combining token and character data into a tf.data Dataset\n",
    "\n",
    "Efficient data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "11657d98-fc70-4b66-8e42-6df222b1f331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loading_pipeline(X, y, batch_size=32):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that creates a dataset in order to improve data loading into the model.\n",
    "    Args:\n",
    "        X: Original data\n",
    "        y: labels\n",
    "        batch_size: default (32)\n",
    "        \n",
    "    returns:\n",
    "        prefetched dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # creating dataset\n",
    "    data = tf.data.Dataset.from_tensor_slices(X)\n",
    "    labels = tf.data.Dataset.from_tensor_slices(y)\n",
    "    dataset = tf.data.Dataset.zip((data, labels))\n",
    "    \n",
    "    # prefetch and batch\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "669e0b5e-3f66-463c-9341-76f05af1fe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "train_ct_dataset = data_loading_pipeline((train_sentences, train_char), train_labels_ohe)\n",
    "val_ct_dataset = data_loading_pipeline((val_sentences, val_char), val_labels_ohe)\n",
    "test_ct_dataset = data_loading_pipeline((test_sentences, test_char), test_labels_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c6868428-4913-432d-8d17-e997cb9bd685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=((TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None)), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking training char/token dataset\n",
    "train_ct_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0e0c3b-9eac-40fd-9772-4d0d12d6a7cb",
   "metadata": {},
   "source": [
    "### Fitting the token and character-level sequence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c6137e3c-cfc3-45c4-b9ce-68e83bba5314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5627/5627 [==============================] - 259s 44ms/step - loss: 0.7448 - accuracy: 0.7130 - val_loss: 0.6331 - val_accuracy: 0.7581\n",
      "Epoch 2/5\n",
      "5627/5627 [==============================] - 234s 42ms/step - loss: 0.6690 - accuracy: 0.7460 - val_loss: 0.6055 - val_accuracy: 0.7694\n",
      "Epoch 3/5\n",
      "5627/5627 [==============================] - 235s 42ms/step - loss: 0.6467 - accuracy: 0.7542 - val_loss: 0.5929 - val_accuracy: 0.7737\n",
      "Epoch 4/5\n",
      "5627/5627 [==============================] - 237s 42ms/step - loss: 0.6324 - accuracy: 0.7594 - val_loss: 0.5874 - val_accuracy: 0.7757\n",
      "Epoch 5/5\n",
      "5627/5627 [==============================] - 242s 43ms/step - loss: 0.6215 - accuracy: 0.7646 - val_loss: 0.5706 - val_accuracy: 0.7825\n"
     ]
    }
   ],
   "source": [
    "# fitting model on tokens and chars\n",
    "history_4 = model_4.fit(train_ct_dataset, epochs=5, steps_per_epoch=len(train_ct_dataset),\n",
    "                       validation_data=val_ct_dataset, validation_steps=len(val_ct_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c4350df7-771b-4c85-8486-ced89af3d7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "942/942 [==============================] - 33s 35ms/step - loss: 0.5929 - accuracy: 0.7763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5928710699081421, 0.7763066291809082]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluating model\n",
    "model_4.evaluate(test_ct_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5afd65c8-1418-42d6-9f57-2f061fc69694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.16020811e-01, 1.25141472e-01, 8.93104672e-02, 3.09305042e-01,\n",
       "        6.02221079e-02],\n",
       "       [1.77840129e-01, 3.76687907e-02, 5.34586787e-01, 1.57739207e-01,\n",
       "        9.21650305e-02],\n",
       "       [3.54285410e-04, 1.35435152e-03, 9.45897520e-01, 3.76383541e-04,\n",
       "        5.20174429e-02],\n",
       "       ...,\n",
       "       [1.47645796e-05, 1.07007995e-02, 4.48738167e-04, 2.34515642e-06,\n",
       "        9.88833427e-01],\n",
       "       [1.22070650e-03, 4.11018208e-02, 3.27747897e-03, 1.54746929e-04,\n",
       "        9.54245269e-01],\n",
       "       [3.75513062e-02, 6.11669242e-01, 1.62736788e-01, 1.78257655e-02,\n",
       "        1.70216918e-01]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictions using token-character (hybrid) model\n",
    "model_4_preds_probs = model_4.predict(test_ct_dataset)\n",
    "model_4_preds_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bd4133e3-6743-426d-8e43-3923936add39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30135,), dtype=int64, numpy=array([0, 2, 2, ..., 4, 4, 1], dtype=int64)>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Format pred_probs into pred labels\n",
    "model_4_preds = tf.argmax(model_4_preds_probs, axis=1)\n",
    "model_4_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "462b5d97-07ad-4523-9724-6ef734e2ba25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 77.63066202090593,\n",
       " 'precision': 0.7724592496427245,\n",
       " 'recall': 0.7763066202090593,\n",
       " 'f1': 0.7722413685875475}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating model results\n",
    "model_4_results = help_me.calculate_results(y_true=test_labels_enc, y_pred=model_4_preds)\n",
    "model_4_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef622677-0459-4a0d-a231-d766f91d8b42",
   "metadata": {},
   "source": [
    "## Model 5:Transfer Learning with pretrained token embeddings + char embeddings + positional embeddings\n",
    "\n",
    "> **Note**: Any engineered features used to train a model need to be available at the test time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90e5ded-79c8-4e49-a39b-84198fa73247",
   "metadata": {},
   "source": [
    "### Creating Positional Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "10b76fac-fb37-4467-aab7-c68213594ece",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     15000\n",
       "1     15000\n",
       "2     15000\n",
       "3     15000\n",
       "4     14992\n",
       "5     14949\n",
       "6     14758\n",
       "7     14279\n",
       "8     13346\n",
       "9     11981\n",
       "10    10041\n",
       "11     7892\n",
       "12     5853\n",
       "13     4152\n",
       "14     2835\n",
       "15     1861\n",
       "16     1188\n",
       "17      751\n",
       "18      462\n",
       "19      286\n",
       "20      162\n",
       "21      101\n",
       "22       66\n",
       "23       33\n",
       "24       22\n",
       "25       14\n",
       "26        7\n",
       "27        4\n",
       "28        3\n",
       "29        1\n",
       "30        1\n",
       "Name: line_number, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quantity of lines for each abstract\n",
    "train_df['line_number'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fc63471a-66b5-40da-8d6d-a8ce2c3d001f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# checking line amount distribution\n",
    "train_df.line_number.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "60de0629-7829-4b5e-b534-c828ad4b5e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding the line quantity data in order to make positional embeddings\n",
    "## to get an array we need to pass '.to_numpy()'\n",
    "train_ln_ohe = tf.one_hot(train_df['line_number'].to_numpy(), depth=15)\n",
    "val_ln_ohe = tf.one_hot(val_df['line_number'].to_numpy(), depth=15)\n",
    "test_ln_ohe = tf.one_hot(test_df['line_number'].to_numpy(), depth=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "93ce1da9-c7bb-462e-b903-6b2b35974f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([180040, 15]),\n",
       " <tf.Tensor: shape=(10, 15), dtype=float32, numpy=\n",
       " array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking ohe data\n",
    "train_ln_ohe.shape, train_ln_ohe[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7ac08477-0e39-443e-94a0-61897a5dca81",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11    24468\n",
       "10    23639\n",
       "12    22113\n",
       "9     19400\n",
       "13    18438\n",
       "14    14610\n",
       "8     12285\n",
       "15    10768\n",
       "7      7464\n",
       "16     7429\n",
       "17     5202\n",
       "6      3353\n",
       "18     3344\n",
       "19     2480\n",
       "20     1281\n",
       "5      1146\n",
       "21      770\n",
       "22      759\n",
       "23      264\n",
       "4       215\n",
       "24      200\n",
       "25      182\n",
       "26       81\n",
       "28       58\n",
       "3        32\n",
       "30       31\n",
       "27       28\n",
       "Name: total_lines, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoding the total lines\n",
    "train_df['total_lines'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f3692ea6-facc-4d1c-8648-2b8623c4e4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# distribution\n",
    "train_df.total_lines.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "214e34e9-193f-46a3-b5e8-b3e47fe18c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking until which number the data has at least 98% of data\n",
    "np.percentile(train_df.total_lines, 98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f84f8873-be39-4361-a754-5550e9e0853a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 20), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ohe the total lines to serve as an input to the model\n",
    "train_tl_ohe = tf.one_hot(train_df.total_lines.to_numpy(), depth=20)\n",
    "val_tl_ohe = tf.one_hot(val_df.total_lines.to_numpy(), depth=20)\n",
    "test_tl_ohe = tf.one_hot(test_df.total_lines.to_numpy(), depth=20)\n",
    "\n",
    "train_tl_ohe[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1df344e-1df0-4f52-842a-285e0af8ede7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2087b827-75e7-4fbb-bdb9-2b03243755c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "estudo",
   "language": "python",
   "name": "estudo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
